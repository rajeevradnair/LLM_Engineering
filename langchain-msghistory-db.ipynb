{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a341d6d",
   "metadata": {},
   "source": [
    "## Installation of required dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c5be603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in ./py312_langchain/lib/python3.12/site-packages (0.3.25)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.58 in ./py312_langchain/lib/python3.12/site-packages (from langchain) (0.3.63)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in ./py312_langchain/lib/python3.12/site-packages (from langchain) (0.3.8)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in ./py312_langchain/lib/python3.12/site-packages (from langchain) (0.3.43)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in ./py312_langchain/lib/python3.12/site-packages (from langchain) (2.11.5)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in ./py312_langchain/lib/python3.12/site-packages (from langchain) (2.0.41)\n",
      "Requirement already satisfied: requests<3,>=2 in ./py312_langchain/lib/python3.12/site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./py312_langchain/lib/python3.12/site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in ./py312_langchain/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./py312_langchain/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in ./py312_langchain/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in ./py312_langchain/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (4.13.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./py312_langchain/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in ./py312_langchain/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.18)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in ./py312_langchain/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in ./py312_langchain/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./py312_langchain/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in ./py312_langchain/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./py312_langchain/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./py312_langchain/lib/python3.12/site-packages (from requests<3,>=2->langchain) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./py312_langchain/lib/python3.12/site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./py312_langchain/lib/python3.12/site-packages (from requests<3,>=2->langchain) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./py312_langchain/lib/python3.12/site-packages (from requests<3,>=2->langchain) (2025.4.26)\n",
      "Requirement already satisfied: anyio in ./py312_langchain/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in ./py312_langchain/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in ./py312_langchain/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.16.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./py312_langchain/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain) (3.0.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./py312_langchain/lib/python3.12/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: langchain-ollama in ./py312_langchain/lib/python3.12/site-packages (0.3.3)\n",
      "Requirement already satisfied: ollama<1.0.0,>=0.4.8 in ./py312_langchain/lib/python3.12/site-packages (from langchain-ollama) (0.5.1)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.60 in ./py312_langchain/lib/python3.12/site-packages (from langchain-ollama) (0.3.63)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.126 in ./py312_langchain/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.60->langchain-ollama) (0.3.43)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in ./py312_langchain/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.60->langchain-ollama) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./py312_langchain/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.60->langchain-ollama) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./py312_langchain/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.60->langchain-ollama) (6.0.2)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in ./py312_langchain/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.60->langchain-ollama) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in ./py312_langchain/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.60->langchain-ollama) (4.13.2)\n",
      "Requirement already satisfied: pydantic>=2.7.4 in ./py312_langchain/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.60->langchain-ollama) (2.11.5)\n",
      "Requirement already satisfied: httpx>=0.27 in ./py312_langchain/lib/python3.12/site-packages (from ollama<1.0.0,>=0.4.8->langchain-ollama) (0.28.1)\n",
      "Requirement already satisfied: anyio in ./py312_langchain/lib/python3.12/site-packages (from httpx>=0.27->ollama<1.0.0,>=0.4.8->langchain-ollama) (4.9.0)\n",
      "Requirement already satisfied: certifi in ./py312_langchain/lib/python3.12/site-packages (from httpx>=0.27->ollama<1.0.0,>=0.4.8->langchain-ollama) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in ./py312_langchain/lib/python3.12/site-packages (from httpx>=0.27->ollama<1.0.0,>=0.4.8->langchain-ollama) (1.0.9)\n",
      "Requirement already satisfied: idna in ./py312_langchain/lib/python3.12/site-packages (from httpx>=0.27->ollama<1.0.0,>=0.4.8->langchain-ollama) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in ./py312_langchain/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.27->ollama<1.0.0,>=0.4.8->langchain-ollama) (0.16.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./py312_langchain/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.60->langchain-ollama) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in ./py312_langchain/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.126->langchain-core<1.0.0,>=0.3.60->langchain-ollama) (3.10.18)\n",
      "Requirement already satisfied: requests<3,>=2 in ./py312_langchain/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.126->langchain-core<1.0.0,>=0.3.60->langchain-ollama) (2.32.3)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in ./py312_langchain/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.126->langchain-core<1.0.0,>=0.3.60->langchain-ollama) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in ./py312_langchain/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.126->langchain-core<1.0.0,>=0.3.60->langchain-ollama) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./py312_langchain/lib/python3.12/site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.60->langchain-ollama) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in ./py312_langchain/lib/python3.12/site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.60->langchain-ollama) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./py312_langchain/lib/python3.12/site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.60->langchain-ollama) (0.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./py312_langchain/lib/python3.12/site-packages (from requests<3,>=2->langsmith<0.4,>=0.1.126->langchain-core<1.0.0,>=0.3.60->langchain-ollama) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./py312_langchain/lib/python3.12/site-packages (from requests<3,>=2->langsmith<0.4,>=0.1.126->langchain-core<1.0.0,>=0.3.60->langchain-ollama) (2.4.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./py312_langchain/lib/python3.12/site-packages (from anyio->httpx>=0.27->ollama<1.0.0,>=0.4.8->langchain-ollama) (1.3.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: python-dotenv in ./py312_langchain/lib/python3.12/site-packages (1.1.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: ipython in ./py312_langchain/lib/python3.12/site-packages (9.2.0)\n",
      "Requirement already satisfied: decorator in ./py312_langchain/lib/python3.12/site-packages (from ipython) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in ./py312_langchain/lib/python3.12/site-packages (from ipython) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in ./py312_langchain/lib/python3.12/site-packages (from ipython) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in ./py312_langchain/lib/python3.12/site-packages (from ipython) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in ./py312_langchain/lib/python3.12/site-packages (from ipython) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in ./py312_langchain/lib/python3.12/site-packages (from ipython) (3.0.51)\n",
      "Requirement already satisfied: pygments>=2.4.0 in ./py312_langchain/lib/python3.12/site-packages (from ipython) (2.19.1)\n",
      "Requirement already satisfied: stack_data in ./py312_langchain/lib/python3.12/site-packages (from ipython) (0.6.3)\n",
      "Requirement already satisfied: traitlets>=5.13.0 in ./py312_langchain/lib/python3.12/site-packages (from ipython) (5.14.3)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in ./py312_langchain/lib/python3.12/site-packages (from jedi>=0.16->ipython) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in ./py312_langchain/lib/python3.12/site-packages (from pexpect>4.3->ipython) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in ./py312_langchain/lib/python3.12/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in ./py312_langchain/lib/python3.12/site-packages (from stack_data->ipython) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in ./py312_langchain/lib/python3.12/site-packages (from stack_data->ipython) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in ./py312_langchain/lib/python3.12/site-packages (from stack_data->ipython) (0.2.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain\n",
    "!pip install langchain-ollama\n",
    "!pip install python-dotenv\n",
    "!pip install ipython"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d599cc",
   "metadata": {},
   "source": [
    "##Message History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5be9e1c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://api.smith.langchain.com'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "#load langsmith connectivity environment variables\n",
    "dotenv=load_dotenv('.env')\n",
    "os.getenv('LANGSMITH_ENDPOINT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b00b29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "chat_with_llama32=ChatOllama(\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    model=\"qwen3\",\n",
    "    temperature=0.9\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa04c89",
   "metadata": {},
   "source": [
    "## Create a chain of runnable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60597c13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableWithMessageHistory(bound=RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  history: RunnableBinding(bound=RunnableLambda(_enter_history), kwargs={}, config={'run_name': 'load_history'}, config_factories=[])\n",
       "}), kwargs={}, config={'run_name': 'insert_history'}, config_factories=[])\n",
       "| RunnableBinding(bound=RunnableLambda(_call_runnable_sync), kwargs={}, config={'run_name': 'check_sync_or_async'}, config_factories=[]), kwargs={}, config={'run_name': 'RunnableWithMessageHistory'}, config_factories=[]), kwargs={}, config={}, config_factories=[], get_session_history=<function get_session_message_history at 0x10b88bc40>, input_messages_key='new_message', history_messages_key='history', history_factory_config=[ConfigurableFieldSpec(id='session_id', annotation=<class 'str'>, name='Session ID', description='Unique identifier for a session.', default='', is_shared=True, dependencies=None)])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "from langchain_core.prompts import  ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import chain\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_community.chat_message_histories import SQLChatMessageHistory\n",
    "\n",
    "#session store and get message history\n",
    "session_store={}\n",
    "def get_session_message_history(sessionID):\n",
    "    return SQLChatMessageHistory(session_id=sessionID, connection_string=\"sqlite:///chathistory.db\")\n",
    "\n",
    "#create chat prompt\n",
    "chat_prompt_template = ChatPromptTemplate([\n",
    "    ('placeholder', \"{history}\"),\n",
    "    ('human', '{new_message}')\n",
    "])\n",
    "\n",
    "#create chain for the chat session\n",
    "chain = chat_prompt_template | chat_with_llama32 | StrOutputParser()\n",
    "\n",
    "#create the runnable to manage message history for the chain\n",
    "runnable_with_message_history = RunnableWithMessageHistory(\n",
    "    chain, \n",
    "    get_session_history=get_session_message_history,\n",
    "    input_messages_key=\"new_message\",\n",
    "    history_messages_key=\"history\"\n",
    ")\n",
    "runnable_with_message_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b6fb6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a session id\n",
    "random.seed(10)\n",
    "session_id=str(random.randint(1,99999))\n",
    "\n",
    "#get_session_message_history(session_id).clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b85e76d0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ArgumentError",
     "evalue": "Could not parse SQLAlchemy URL from given URL string",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mArgumentError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#begin the chat\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m response1 = \u001b[43mrunnable_with_message_history\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnew_message\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mWhat is the benefit of running LLM in local machine?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mconfigurable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msession_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msession_id\u001b[49m\u001b[43m}\u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(response1)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/rn/langchain/py312_langchain/lib/python3.12/site-packages/langchain_core/runnables/base.py:5432\u001b[39m, in \u001b[36mRunnableBindingBase.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   5423\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   5424\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m   5425\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   5428\u001b[39m     **kwargs: Optional[Any],\n\u001b[32m   5429\u001b[39m ) -> Output:\n\u001b[32m   5430\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.bound.invoke(\n\u001b[32m   5431\u001b[39m         \u001b[38;5;28minput\u001b[39m,\n\u001b[32m-> \u001b[39m\u001b[32m5432\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m   5433\u001b[39m         **{**\u001b[38;5;28mself\u001b[39m.kwargs, **kwargs},\n\u001b[32m   5434\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/rn/langchain/py312_langchain/lib/python3.12/site-packages/langchain_core/runnables/history.py:596\u001b[39m, in \u001b[36mRunnableWithMessageHistory._merge_configs\u001b[39m\u001b[34m(self, *configs)\u001b[39m\n\u001b[32m    593\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(expected_keys) == \u001b[32m1\u001b[39m:\n\u001b[32m    594\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m parameter_names:\n\u001b[32m    595\u001b[39m         \u001b[38;5;66;03m# If arity = 1, then invoke function by positional arguments\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m596\u001b[39m         message_history = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_session_history\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    597\u001b[39m \u001b[43m            \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[43m[\u001b[49m\u001b[43mexpected_keys\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    598\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    599\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    600\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m config:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mget_session_message_history\u001b[39m\u001b[34m(sessionID)\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_session_message_history\u001b[39m(sessionID):\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSQLChatMessageHistory\u001b[49m\u001b[43m(\u001b[49m\u001b[43msession_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43msessionID\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconnection_string\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msqlite.///chathistory.db\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/rn/langchain/py312_langchain/lib/python3.12/site-packages/langchain_community/chat_message_histories/sql.py:202\u001b[39m, in \u001b[36mSQLChatMessageHistory.__init__\u001b[39m\u001b[34m(self, session_id, connection_string, table_name, session_id_field_name, custom_message_converter, connection, engine_args, async_mode)\u001b[39m\n\u001b[32m    198\u001b[39m         \u001b[38;5;28mself\u001b[39m.async_engine = create_async_engine(\n\u001b[32m    199\u001b[39m             connection, **(engine_args \u001b[38;5;129;01mor\u001b[39;00m {})\n\u001b[32m    200\u001b[39m         )\n\u001b[32m    201\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m202\u001b[39m         \u001b[38;5;28mself\u001b[39m.engine = \u001b[43mcreate_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(connection, Engine):\n\u001b[32m    204\u001b[39m     \u001b[38;5;28mself\u001b[39m.async_mode = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:2\u001b[39m, in \u001b[36mcreate_engine\u001b[39m\u001b[34m(url, **kwargs)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/rn/langchain/py312_langchain/lib/python3.12/site-packages/sqlalchemy/util/deprecations.py:281\u001b[39m, in \u001b[36mdeprecated_params.<locals>.decorate.<locals>.warned\u001b[39m\u001b[34m(fn, *args, **kwargs)\u001b[39m\n\u001b[32m    274\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[32m    275\u001b[39m         _warn_with_version(\n\u001b[32m    276\u001b[39m             messages[m],\n\u001b[32m    277\u001b[39m             versions[m],\n\u001b[32m    278\u001b[39m             version_warnings[m],\n\u001b[32m    279\u001b[39m             stacklevel=\u001b[32m3\u001b[39m,\n\u001b[32m    280\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m281\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/rn/langchain/py312_langchain/lib/python3.12/site-packages/sqlalchemy/engine/create.py:549\u001b[39m, in \u001b[36mcreate_engine\u001b[39m\u001b[34m(url, **kwargs)\u001b[39m\n\u001b[32m    546\u001b[39m kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mempty_in_strategy\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    548\u001b[39m \u001b[38;5;66;03m# create url.URL object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m549\u001b[39m u = \u001b[43m_url\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmake_url\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    551\u001b[39m u, plugins, kwargs = u._instantiate_plugins(kwargs)\n\u001b[32m    553\u001b[39m entrypoint = u._get_entrypoint()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/rn/langchain/py312_langchain/lib/python3.12/site-packages/sqlalchemy/engine/url.py:856\u001b[39m, in \u001b[36mmake_url\u001b[39m\u001b[34m(name_or_url)\u001b[39m\n\u001b[32m    840\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Given a string, produce a new URL instance.\u001b[39;00m\n\u001b[32m    841\u001b[39m \n\u001b[32m    842\u001b[39m \u001b[33;03mThe format of the URL generally follows `RFC-1738\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    852\u001b[39m \n\u001b[32m    853\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    855\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(name_or_url, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m856\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_parse_url\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    857\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(name_or_url, URL) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\n\u001b[32m    858\u001b[39m     name_or_url, \u001b[33m\"\u001b[39m\u001b[33m_sqla_is_testing_if_this_is_a_mock_object\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    859\u001b[39m ):\n\u001b[32m    860\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc.ArgumentError(\n\u001b[32m    861\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected string or URL object, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname_or_url\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    862\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/rn/langchain/py312_langchain/lib/python3.12/site-packages/sqlalchemy/engine/url.py:922\u001b[39m, in \u001b[36m_parse_url\u001b[39m\u001b[34m(name)\u001b[39m\n\u001b[32m    919\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m URL.create(name, **components)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    921\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m922\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc.ArgumentError(\n\u001b[32m    923\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCould not parse SQLAlchemy URL from given URL string\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    924\u001b[39m     )\n",
      "\u001b[31mArgumentError\u001b[39m: Could not parse SQLAlchemy URL from given URL string"
     ]
    }
   ],
   "source": [
    "#begin the chat\n",
    "response1 = runnable_with_message_history.invoke(\n",
    "    {\"new_message\":\"What is the benefit of running LLM in local machine?\"},\n",
    "    config={\"configurable\":{\"session_id\": session_id}}\n",
    ")\n",
    "print(response1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6704ae5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, the user asked about the benefits of running LLMs on local machines, and now they're asking \"How about cloud?\" So they want to know the advantages of using the cloud instead. Let me start by recalling the previous answer where I listed the pros of local deployment.\n",
      "\n",
      "I need to structure the cloud benefits similarly but make sure to contrast where appropriate. Let me think about the main points. Cloud computing offers scalability, cost efficiency through pay-as-you-go, access to advanced infrastructure, and maybe better support for training large models. Also, collaboration and integration with other services could be a point.\n",
      "\n",
      "Wait, the user might be comparing the two approaches. They might be deciding between on-premise and cloud. So I should highlight the pros of cloud while acknowledging that it's not always the best choice. Maybe mention scenarios where cloud is better, like when you need to train models, handle variable workloads, or don't have the hardware.\n",
      "\n",
      "I should also consider the trade-offs of cloud, like dependency on internet, potential data privacy concerns, and costs for high usage. But the user specifically asked about the cloud benefits, so focus on that. Let me make sure to cover scalability, cost, infrastructure, ease of use, and integration with other services. Also, mention that cloud is suitable for training, while local is better for inference in sensitive environments.\n",
      "\n",
      "I need to ensure the answer is balanced, explaining when cloud is advantageous and perhaps why someone might prefer it over local. Maybe add examples of use cases where cloud is ideal, like large-scale training or handling fluctuating workloads. Also, touch on the support and managed services that cloud providers offer, which can reduce the operational burden.\n",
      "</think>\n",
      "\n",
      "Running a large language model (LLM) on the **cloud** offers distinct advantages, especially for scenarios that require **scalability, flexibility, and access to advanced infrastructure**. Here's a structured breakdown of the benefits of cloud-based deployment:\n",
      "\n",
      "---\n",
      "\n",
      "### **1. Scalability and Flexibility**\n",
      "- **Dynamic Resource Allocation**: Cloud platforms (e.g., AWS, Azure, Google Cloud) allow you to scale compute resources (GPUs/TPUs) up or down based on workload demands, such **training large models** or handling **spike in inference requests**.\n",
      "- **Elasticity**: Easily expand infrastructure to accommodate growing data or user traffic without overprovisioning hardware.\n",
      "\n",
      "---\n",
      "\n",
      "### **2. Cost Efficiency for Training and High-Volume Workloads**\n",
      "- **Pay-as-you-go Model**: Cloud providers charge based on usage (e.g., compute hours, storage), reducing upfront costs for training models or handling variable workloads.\n",
      "- **Optimized for Training**: Cloud infrastructure is designed for distributed training of large models (e.g., using frameworks like Horovod or DeepSpeed), offering access to massive compute clusters and specialized hardware (e.g., A100 GPUs).\n",
      "\n",
      "---\n",
      "\n",
      "### **3. Advanced Infrastructure and Tools**\n",
      "- **Pre-Configured Environments**: Cloud platforms offer managed services (e.g., Amazon SageMaker, Google AI Platform) with pre-installed frameworks, libraries, and tools for training and deploying LLMs.\n",
      "- **Specialized Hardware**: Access to high-performance GPUs/TPUs (e.g., NVIDIA A100, Google TPU) and optimized software stacks (e.g., TensorFlow, PyTorch) for faster training and inference.\n",
      "\n",
      "---\n",
      "\n",
      "### **4. Collaboration and Integration**\n",
      "- **Team Collaboration**: Cloud platforms enable seamless collaboration by hosting models, datasets, and workflows in centralized environments, accessible by teams worldwide.\n",
      "- **Integration with Ecosystems**: Easily integrate LLMs with other cloud services (e.g., databases, APIs, storage) for end-to-end applications (e.g., chatbots, data analysis pipelines).\n",
      "\n",
      "---\n",
      "\n",
      "### **5. Reduced Maintenance and Operational Overhead**\n",
      "- **Managed Services**: Cloud providers handle infrastructure management, updates, and security patches, reducing the operational burden on in-house teams.\n",
      "- **Auto-Scaling and Monitoring**: Built-in tools for monitoring performance, autoscaling resources, and troubleshooting, which are critical for large-scale deployments.\n",
      "\n",
      "---\n",
      "\n",
      "### **6. Access to Global Data and Compute Resources**\n",
      "- **Data Accessibility**: Cloud platforms allow access to vast datasets and distributed computing resources, enabling training on diverse and large-scale data.\n",
      "- **Global Reach**: Deploy models globally with low-latency access to users worldwide, ideal for international applications.\n",
      "\n",
      "---\n",
      "\n",
      "### **7. Use Case-Specific Advantages**\n",
      "- **Training Large Models**: Cloud is ideal for training massive LLMs (e.g., GPT-3, LLaMA) due to its scalability and specialized hardware.\n",
      "- **High-Volume Inference**: Cloud services can handle thousands of requests per second, making them suitable for applications like customer service chatbots or real-time translation.\n",
      "- **Hybrid Cloud Solutions**: Combine cloud and local deployments (e.g., using edge computing for inference and cloud for training) to balance performance, cost, and security.\n",
      "\n",
      "---\n",
      "\n",
      "### **Trade-Offs of Cloud Deployment**\n",
      "- **Internet Dependency**: Requires stable internet connectivity for model inference and data transfer.\n",
      "- **Data Privacy Risks**: Sensitive data processed in the cloud may face exposure risks, though encrypted data pipelines and compliance tools mitigate this.\n",
      "- **Cost for High Usage**: While scalable, heavy workloads (e.g., training large models) can become expensive if not optimized.\n",
      "\n",
      "---\n",
      "\n",
      "### **When to Choose Cloud vs. Local**\n",
      "- **Cloud**: Best for **training large models**, **high-volume inference**, **collaborative workflows**, and **global scalability**.\n",
      "- **Local**: Better for **data privacy**, **offline operation**, **customization**, and **cost efficiency for small-scale or sensitive workloads**.\n",
      "\n",
      "---\n",
      "\n",
      "### **Examples of Cloud Use Cases**\n",
      "- **Research and Development**: Training cutting-edge models (e.g., GPT-4, PaLM) using cloud's distributed compute.\n",
      "- **Enterprise Applications**: Deploying LLMs for customer support, content generation, or analytics with cloud integration.\n",
      "- **MLOps Pipelines**: Automating model training, deployment, and monitoring using cloud-native tools (e.g., MLflow, Kubeflow).\n",
      "\n",
      "---\n",
      "\n",
      "### **Conclusion**\n",
      "The **cloud** is ideal for **scalability, training large models, and handling high-demand workloads**, while **local deployment** excels in **privacy, customization, and offline access**. The choice depends on factors like **budget, data sensitivity, infrastructure control**, and **specific use case requirements**. For many organizations, a **hybrid approach** (cloud for training, local for inference) often provides the best balance of benefits.\n"
     ]
    }
   ],
   "source": [
    "#new prompt on the chat\n",
    "response2 = runnable_with_message_history.invoke(\n",
    "    {\"new_message\":\"How about cloud?\"},\n",
    "    config={\"configurable\":{\"session_id\": session_id}}\n",
    ")\n",
    "print(response2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312_langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
